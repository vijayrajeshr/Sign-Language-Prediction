# Sign Language Prediction ğŸ¤Ÿ

A deep learning-based system for real-time American Sign Language (ASL) alphabet recognition using a webcam and Convolutional Neural Networks (CNN).

## ğŸš€ Overview

This project enables live ASL hand gesture recognition, translating static signs into text using a custom-trained CNN model. It includes tools for data collection, preprocessing, training, and real-time inference.

## ğŸ“ Repository Structure

- `collect_imgs.py` â€“ Capture labeled gesture images via webcam.
- `create_dataset.py` â€“ Process and convert images into a structured dataset.
- `train_classifier.py` â€“ Train the CNN classifier.
- `inference_classifier.py` â€“ Perform real-time ASL prediction.
- `graph.py` â€“ Visualize training metrics (accuracy/loss).
- `model.p` â€“ Saved trained model.
- `data.pickle` â€“ Serialized labels and data features.
- `LICENSE` â€“ Project license information.

## ğŸ“˜ Documentation

Detailed explanation of the pipeline, architecture, and methodology is available in the [`sign-language-prediction.pdf`](./sign-language-prediction.pdf).

## ğŸ“„ License

This project is licensed under the [MIT License](./LICENSE) ğŸ“

Questions and doubts are welcomed here.
